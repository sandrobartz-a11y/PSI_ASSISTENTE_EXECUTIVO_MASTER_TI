{"nbformat": 4, "nbformat_minor": 5, "metadata": {"kernelspec": {"name": "python3", "display_name": "Python 3", "language": "python"}, "language_info": {"name": "python", "version": "3.10"}}, "cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Produ√ß√£o Tem√°tica PSI ‚Äì Assistente Executivo Master\n", "### Tema: Empr√©stimos e Contratos Financeiros (Open Finance / CSVs PSI)\n", "**Autor:** Sandro Kazanoski Bartz  \\\n", "**Notebook (.ipynb) ‚Äì compat√≠vel com Databricks**  \\\n", "**Data de gera√ß√£o:** 2026-01-29 01:51:31\\n", "\n", "> Este notebook foi preparado para rodar em **Databricks** (pyspark dispon√≠vel) **ou** localmente (fallback com *SparkSession* local / Pandas). Inclui: limpeza, checagens de qualidade, modelagem anal√≠tica, EDA, constru√ß√£o de *features*, KPIs e *storytelling*."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Objetivos do desafio\n", "- Carregar e compreender os dados de empr√©stimos/contratos.\n", "- Realizar **limpeza** e **verifica√ß√£o de qualidade**.\n", "- Construir **tabelas curadas** (camada *silver/gold*) para an√°lise.\n", "- Identificar **padr√µes/tend√™ncias** e extrair **insights de neg√≥cio**.\n", "- Entregar **visualiza√ß√µes** e **storytelling** para gestores.\n", "\n", "### Arquivos esperados (CSV)\n", "- `psi_convenentes_2026-01-13_17-09-53.csv`\n", "- `psi_emprestimo_contrato_2026-01-13_17-09-53.csv`\n", "- `psi_emprestimo_contrato_encargo_financeiro_2026-01-13_17-09-53.csv`\n", "- `psi_emprestimo_contrato_tarifa_2026-01-13_17-09-53.csv`\n", "- `psi_emprestimo_contrato_taxa_juros_2026-01-13_17-09-53.csv`\n", "- `psi_emprestimo_garantia_2026-01-13_17-09-53.csv`\n", "- `psi_emprestimo_pagamento_2026-01-15.csv`\n", "- `psi_emprestimo_pagamento_lancamento_2026-01-15.csv`\n", "- `psi_emprestimo_parcela_programada_2026-01-15.csv`\n", "\n", "> **Observa√ß√£o:** o notebook √© resiliente a pequenas varia√ß√µes de esquema: detecta cabe√ßalhos, usa delimitador `;` e tenta *inferSchema* com tratamento de datas t√≠picas (ISO/BR)."]}, {"cell_type": "code", "metadata": {"collapsed": false}, "execution_count": null, "outputs": [], "source": ["# ‚õèÔ∏è Setup b√°sico (funciona em Databricks e fora)\n", "import os, sys, math, json, textwrap, datetime as dt\n", "from typing import List, Dict\n", "\n", "try:\n", "    spark\n", "except NameError:\n", "    # Se n√£o estiver em Databricks, cria uma SparkSession local\n", "    from pyspark.sql import SparkSession\n", "    spark = SparkSession.builder.appName(\"PSI_Analise_Emprestimos\").getOrCreate()\n", "\n", "from pyspark.sql import functions as F, types as T\n", "\n", "print(f'Spark versao: {spark.version}')\n", "\n", "# Widgets (Databricks) - tolerante a aus√™ncia do dbutils\n", "def get_widget_or_env(name, default):\n", "    try:\n", "        dbutils.widgets.text(name, default)\n", "        return dbutils.widgets.get(name)\n", "    except Exception:\n", "        return os.environ.get(name, default)\n", "\n", "BASE_PATH = get_widget_or_env('BASE_PATH', '.')\n", "DATE_START = get_widget_or_env('DATE_START', '2018-01-01')\n", "DATE_END   = get_widget_or_env('DATE_END',   '2036-12-31')\n", "print('BASE_PATH =', BASE_PATH)\n", "print('Intervalo =', DATE_START, '‚Üí', DATE_END)\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# üìÑ Mapeamento de arquivos CSV\n", "files = {\n", "  'convenentes': 'psi_convenentes_2026-01-13_17-09-53.csv',\n", "  'contrato'  : 'psi_emprestimo_contrato_2026-01-13_17-09-53.csv',\n", "  'encargo'   : 'psi_emprestimo_contrato_encargo_financeiro_2026-01-13_17-09-53.csv',\n", "  'tarifa'    : 'psi_emprestimo_contrato_tarifa_2026-01-13_17-09-53.csv',\n", "  'taxa'      : 'psi_emprestimo_contrato_taxa_juros_2026-01-13_17-09-53.csv',\n", "  'garantia'  : 'psi_emprestimo_garantia_2026-01-13_17-09-53.csv',\n", "  'pagamento' : 'psi_emprestimo_pagamento_2026-01-15.csv',\n", "  'lancamento': 'psi_emprestimo_pagamento_lancamento_2026-01-15.csv',\n", "  'parcela'   : 'psi_emprestimo_parcela_programada_2026-01-15.csv'\n", "}\n", "\n", "def path(name):\n", "    return os.path.join(BASE_PATH, files[name])\n", "\n", "files\n"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# üì• Fun√ß√µes utilit√°rias de leitura com tratamento padr√£o (delimitador ';', header, inferSchema)\n", "def read_csv_semicolon(fp):\n", "    return (spark.read\n", "            .option('header','true')\n", "            .option('delimiter',';')\n", "            .option('inferSchema','true')\n", "            .csv(fp))\n", "\n", "dfs = {}\n", "for k in files:\n", "    fp = path(k)\n", "    try:\n", "        df = read_csv_semicolon(fp)\n", "        dfs[k] = df\n", "        print(k, '‚Üí', df.count(), 'linhas |', len(df.columns), 'colunas')\n", "    except Exception as e:\n", "        print(f'AVISO: falha ao ler {fp}: {e}')\n", "\n", "# Exibe schemas detectados (amostras)\n", "for k, df in dfs.items():\n", "    print('\nSchema:', k)\n", "    df.printSchema()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Qualidade dos Dados (DQ)\n", "Ser√£o aplicados *checks* essenciais: nulidade, duplicidade de chaves, tipos e faixas v√°lidas, datas inconsistentes e reconcilia√ß√µes simples."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["# üîç Fun√ß√µes de DQ\n", "def null_report(df, name):\n", "    rows = df.count()\n", "    exprs = [F.sum(F.when(F.col(c).isNull() | F.col(c).isin('','null','NULL'), 1).otherwise(0)).alias(c) for c in df.columns]\n", "    nr = df.select(exprs).collect()[0].asDict()\n", "    out = [(k, int(v), round(int(v)/rows,4) if rows else 0) for k,v in nr.items()]\n", "    return spark.createDataFrame(out, ['coluna','n_null','perc_null'])\n", "\n", "def dup_count(df, cols:List[str]):\n", "    return df.groupBy([F.col(c) for c in cols]).count().filter('count>1')\n", "\n", "# Exemplos de chaves prov√°veis\n", "key_contrato = ['contractId'] if 'contrato' in dfs and 'contractId' in dfs['contrato'].columns else None\n", "key_pagto    = ['paymentId'] if 'pagamento' in dfs and 'paymentId' in dfs['pagamento'].columns else None\n", "key_parcela  = ['contractId','instalmentNumber'] if 'parcela' in dfs else None\n", "\n", "reports = {}\n", "for name, df in dfs.items():\n", "    reports[name] = null_report(df, name)\n", "    print(f'Relat√≥rio de nulos: {name}')\n", "    reports[name].orderBy(F.desc('perc_null')).show(10, truncate=False)\n", "\n", "if key_contrato:\n", "    print('Duplicatas em contrato:')\n", "    dup_count(dfs['contrato'], key_contrato).show(5, False)\n", "if key_parcela and set(key_parcela).issubset(set(dfs['parcela'].columns)):\n", "    print('Duplicatas em parcela:')\n", "    dup_count(dfs['parcela'], key_parcela).show(5, False)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Limpeza e Padroniza√ß√µes\n", "- *Trim* e *upper* em campos de texto de classifica√ß√£o (produto, tipo/subtipo).\n", "- Convers√£o de datas (`contractDate`, `dueDate`, `firstInstalmentDueDate`, etc.).\n", "- Normaliza√ß√£o de valores num√©ricos (substitui v√≠rgula por ponto se necess√°rio).\n", "- Harmoniza√ß√£o de chaves (`contractId`, `ipocCode`)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def to_date(col):\n", "    # Tenta m√∫ltiplos formatos\n", "    return F.coalesce(\n", "        F.to_date(col, 'yyyy-MM-dd'),\n", "        F.to_date(col, 'yyyy-MM-dd'T'HH:mm:ss.SSS'Z''),\n", "        F.to_date(col, 'dd/MM/yyyy'),\n", "        F.to_date(col, 'dd-MM-yyyy')\n", "    )\n", "\n", "def normalize_numeric(c):\n", "    return F.regexp_replace(F.regexp_replace(F.col(c), ',', '.'), ' ', '').cast('double')\n", "\n", "contrato = dfs.get('contrato')\n", "if contrato is not None:\n", "    cols_up = [c for c in ['productName','productType','productSubType','productSubTypeCategory','amortizationScheduled','instalmentPeriodicity','currency','nomeInstituicao'] if c in contrato.columns]\n", "    for c in cols_up:\n", "        contrato = contrato.withColumn(c, F.upper(F.trim(F.col(c))))\n", "    # Datas\n", "    for c in ['contractDate','settlementDate','dueDate','firstInstalmentDueDate']:\n", "        if c in contrato.columns:\n", "            contrato = contrato.withColumn(c, to_date(F.col(c)))\n", "    # Num√©ricos\n", "    for c in ['contractAmount','CET','nextInstalmentAmount']:\n", "        if c in contrato.columns:\n", "            contrato = contrato.withColumn(c, normalize_numeric(c))\n", "    dfs['contrato'] = contrato.cache()\n", "    print('Contrato limpo:', contrato.count(), 'linhas')\n", "\n", "# Limpeza similar para parcelas e pagamentos (se existirem)\n", "for name in ['parcela','pagamento','lancamento','encargo','tarifa','taxa']:\n", "    df = dfs.get(name)\n", "    if df is None:\n", "        continue\n", "    for c in df.columns:\n", "        if df.schema[c].dataType.simpleString() == 'string':\n", "            df = df.withColumn(c, F.trim(F.col(c)))\n", "    # tentativa gen√©rica de converter campos de data conhecidos\n", "    date_like = [c for c in df.columns if 'date' in c.lower() or 'data' in c.lower()]\n", "    for c in date_like:\n", "        df = df.withColumn(c, to_date(F.col(c)))\n", "    dfs[name] = df.cache()\n", "    print(f'{name} limpo:', df.count(), 'linhas')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Modelo Anal√≠tico (Curado)\n", "Construiremos *facts* e *dims*:\n", "- **dim_convenente** (setor, prazos e faixas)\n", "- **dim_produto** (tipo/subtipo/categoria/amortiza√ß√£o/periodicidade)\n", "- **f_contrato** (gr√£o: `contractId`)\n", "- **f_parcela_programada** (gr√£o: `contractId` + n¬∫ parcela)\n", "- **f_pagamento** e **f_lancamento** (fluxos)\n", "- *lookups* de **tarifa/encargo/taxa/garantia** ligadas ao contrato."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["contrato = dfs.get('contrato')\n", "conven   = dfs.get('convenentes')\n", "parcela  = dfs.get('parcela')\n", "pagto    = dfs.get('pagamento')\n", "lcto     = dfs.get('lancamento')\n", "encargo  = dfs.get('encargo')\n", "tarifa   = dfs.get('tarifa')\n", "taxa     = dfs.get('taxa')\n", "garantia = dfs.get('garantia')\n", "\n", "# dim_produto\n", "dim_produto = None\n", "if contrato is not None:\n", "    attrs = [c for c in ['productName','productType','productSubType','productSubTypeCategory','amortizationScheduled','instalmentPeriodicity'] if c in contrato.columns]\n", "    dim_produto = (contrato\n", "        .select(attrs)\n", "        .dropDuplicates()\n", "        .withColumn('produto_id', F.sha2(F.concat_ws('||', *[F.coalesce(F.col(c), F.lit('')) for c in attrs]), 256))\n", "    )\n", "\n", "# dim_convenente\n", "dim_convenente = None\n", "if conven is not None:\n", "    csel = [c for c in ['convenente','cpf_cnpj','codigo_situacao','situacao','setor','prazo_inicial','prazo_final','faixa_b','faixa_c'] if c in conven.columns]\n", "    dim_convenente = (conven\n", "        .select(csel)\n", "        .withColumnRenamed('cpf_cnpj','cpfCnpj_convenente')\n", "        .withColumn('convenente_id', F.sha2(F.concat_ws('||', *[F.coalesce(F.col(c), F.lit('')) for c in csel]), 256))\n", "        .dropDuplicates())\n", "\n", "# f_contrato\n", "f_contrato = None\n", "if contrato is not None:\n", "    f_contrato = contrato\n", "    if dim_produto is not None:\n", "        f_contrato = (f_contrato\n", "            .join(dim_produto, on=attrs, how='left'))\n", "    # chave convenente por cpfCnpj quando houver\n", "    if dim_convenente is not None and 'cpfCnpj' in f_contrato.columns:\n", "        f_contrato = (f_contrato\n", "            .join(dim_convenente.select('convenente_id','cpfCnpj_convenente'),\n", "                  f_contrato.cpfCnpj == dim_convenente.cpfCnpj_convenente, 'left'))\n", "    # Janela para prazo do contrato em meses (se dueDate/contractDate dispon√≠veis)\n", "    if set(['contractDate','dueDate']).issubset(set(f_contrato.columns)):\n", "        f_contrato = f_contrato.withColumn('prazo_meses_est',\n", "            F.floor(F.months_between(F.col('dueDate'), F.col('contractDate'))))\n", "\n", "# f_parcela_programada\n", "f_parcela = None\n", "if parcela is not None:\n", "    f_parcela = parcela\n", "\n", "# f_pagamento e f_lancamento\n", "f_pagamento = pagto if pagto is not None else None\n", "f_lancamento = lcto if lcto is not None else None\n", "\n", "# Links auxiliares (tarifa/encargo/taxa/garantia) agregadas por contrato\n", "def agg_by_contract(df, value_cols:List[str], prefix:str):\n", "    sels = ['contractId'] + [c for c in value_cols if c in df.columns]\n", "    aggs = [F.sum(F.col(c)).alias(f'{prefix}_{c}_sum') for c in value_cols if c in df.columns]\n", "    return df.select(*sels).groupBy('contractId').agg(*aggs)\n", "\n", "agg_list = []\n", "if encargo is not None:\n", "    cols = [c for c in ['valor','valorCalculado','valorOriginal','valorAtual'] if c in encargo.columns]\n", "    agg_list.append(agg_by_contract(encargo, cols, 'encargo'))\n", "if tarifa is not None:\n", "    cols = [c for c in ['valor','valorCalculado','valorOriginal','valorAtual'] if c in tarifa.columns]\n", "    agg_list.append(agg_by_contract(tarifa, cols, 'tarifa'))\n", "if taxa is not None:\n", "    cols = [c for c in ['taxaNominal','taxaEfetiva','taxaAnual'] if c in taxa.columns]\n", "    agg_list.append(agg_by_contract(taxa, cols, 'taxa'))\n", "if garantia is not None:\n", "    g_cnt = garantia.groupBy('contractId').count().withColumnRenamed('count','qtd_garantias')\n", "    agg_list.append(g_cnt)\n", "\n", "# Enriquecimento do contrato\n", "if f_contrato is not None:\n", "    for agg in agg_list:\n", "        f_contrato = f_contrato.join(agg, on='contractId', how='left')\n", "    f_contrato = f_contrato.cache()\n", "    print('f_contrato pronto:', f_contrato.count(), 'linhas')\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## KPIs e Indicadores\n", "- **Carteira (R$)**: soma de `contractAmount`.\n", "- **CET m√©dio** por produto/subtipo/categoria.\n", "- **Prazo m√©dio** (meses).\n", "- **Intensidade de encargos/tarifas** por contrato.\n", "- **Uso de garantias** (# por contrato).\n", "- **Adimpl√™ncia/Inadimpl√™ncia** (*proxy* via pagamentos atrasados, se campos dispon√≠veis)."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["from pyspark.sql.window import Window\n", "\n", "if f_contrato is not None:\n", "    print('Carteira total (R$):')\n", "    f_contrato.select(F.sum('contractAmount').alias('carteira_total')).show()\n", "\n", "    kpi = f_contrato\n", "    if 'productSubType' in kpi.columns:\n", "        print('CET m√©dio por subproduto:')\n", "        kpi.groupBy('productSubType').agg(F.avg('CET').alias('CET_medio'), F.count('*').alias('qtd')).orderBy(F.desc('qtd')).show(20, False)\n", "\n", "    if 'prazo_meses_est' in kpi.columns:\n", "        print('Prazo m√©dio (meses):')\n", "        kpi.select(F.avg('prazo_meses_est')).show()\n", "\n", "# Proxy simples de atraso se existirem datas de parcela programada e pagamento\n", "if 'parcela' in dfs and 'pagamento' in dfs:\n", "    parc = dfs['parcela']\n", "    pay  = dfs['pagamento']\n", "    # tenta chaves padr√£o\n", "    join_cols = [c for c in ['contractId','instalmentNumber'] if c in parc.columns and c in pay.columns]\n", "    if join_cols:\n", "        pago = (parc.alias('p')\n", "            .join(pay.alias('y'), on=join_cols, how='left'))\n", "        # atraso (dias) se houver dueDate/dtPagamento\n", "        for dcol in ['dueDate','dataVencimento','dataVenc']:\n", "            if dcol in pago.columns:\n", "                duec = dcol\n", "                break\n", "        else:\n", "            duec = None\n", "        for pcol in ['paymentDate','dataPagamento','dtPagamento']:\n", "            if pcol in pago.columns:\n", "                payc = pcol\n", "                break\n", "        else:\n", "            payc = None\n", "        if duec and payc:\n", "            pago = pago.withColumn('dias_atraso', F.datediff(F.col(payc), F.col(duec)))\n", "            print('Distribui√ß√£o de atraso (amostra):')\n", "            pago.groupBy(F.when(F.col('dias_atraso')>0, 'ATRASO').otherwise('EM DIA').alias('status')).count().show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Visualiza√ß√µes (r√°pidas)\n", "> Em Databricks, use `display()` nos *DataFrames*. Abaixo, exemplo com Pandas/Matplotlib para exportar gr√°ficos simples."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["import pandas as pd\n", "import matplotlib.pyplot as plt\n", "plt.rcParams['figure.figsize'] = (10,5)\n", "plt.rcParams['axes.titlesize'] = 12\n", "\n", "if f_contrato is not None:\n", "    top_prod = (f_contrato\n", "        .groupBy('productSubType')\n", "        .agg(F.sum('contractAmount').alias('carteira'))\n", "        .orderBy(F.desc('carteira'))\n", "        .limit(10))\n", "    pdf = top_prod.toPandas()\n", "    pdf.plot(kind='bar', x='productSubType', y='carteira', legend=False)\n", "    plt.title('Top 10 Subprodutos por Carteira (R$)')\n", "    plt.ylabel('Carteira (R$)')\n", "    plt.xlabel('Subproduto')\n", "    plt.tight_layout()\n", "    plt.show()\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Storytelling (Template)\n", "**Pergunta de neg√≥cio:** Como otimizar a carteira de cr√©dito pessoal (com/sem consigna√ß√£o) maximizando margem com risco controlado?\n", "\n", "**Principais achados (preencha com a sua execu√ß√£o dos KPIs acima):**\n", "1. Subprodutos com maior CET m√©dio e *take-up* de garantias indicam ...\n", "2. Prazo m√©dio de X meses com concentra√ß√£o entre Y‚ÄìZ meses sugere ...\n", "3. Segmentos de convenentes (setor p√∫blico x INSS) exibem ...\n", "4. Proxy de atraso indica taxa de inadimpl√™ncia de ~A% em N contratos ...\n", "\n", "**Oportunidade de neg√≥cio (exemplo):**\n", "- Ajustar *pricing* (CET) e *mix* de prazos em subprodutos com maior propens√£o a atraso;\n", "- Priorizar origina√ß√£o em conv√™nios com *spread* e *loss given default* mais favor√°veis;\n", "- Melhorar *onboarding* documental e checagens de elegibilidade para reduzir custos de tarifa/encargo."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Persist√™ncia (Delta/Parquet)\n", "No Databricks, recomenda-se salvar em **Delta**. Abaixo, usamos Parquet como *fallback*."]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["OUTPUT_PATH = os.path.join(BASE_PATH, 'curated')\n", "dbutils_fs_available = 'dbutils' in globals() and hasattr(dbutils, 'fs')\n", "if dbutils_fs_available:\n", "    try:\n", "        dbutils.fs.mkdirs(f'file:{OUTPUT_PATH}')\n", "    except Exception:\n", "        pass\n", "else:\n", "    os.makedirs(OUTPUT_PATH, exist_ok=True)\n", "\n", "to_save = {'f_contrato': 'f_contrato', 'dim_produto': 'dim_produto', 'dim_convenente': 'dim_convenente'}\n", "for name, df in [('f_contrato', globals().get('f_contrato')), ('dim_produto', globals().get('dim_produto')), ('dim_convenente', globals().get('dim_convenente'))]:\n", "    if df is None:\n", "        continue\n", "    dest = os.path.join(OUTPUT_PATH, name)\n", "    print('Salvando', name, 'em', dest)\n", "    try:\n", "        df.write.mode('overwrite').parquet(dest)\n", "    except Exception as e:\n", "        print('Falha ao salvar', name, e)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Dicion√°rio de Dados (gerado automaticamente a partir dos *DataFrames*)"]}, {"cell_type": "code", "metadata": {}, "execution_count": null, "outputs": [], "source": ["def schema_table(df, name):\n", "    rows = [(f.name, f.dataType.simpleString(), f.nullable) for f in df.schema.fields]\n", "    return spark.createDataFrame(rows, ['coluna','tipo','aceita_nulo']).withColumn('tabela', F.lit(name))\n", "\n", "tab_list = []\n", "for name, df in dfs.items():\n", "    try:\n", "        tab_list.append(schema_table(df, name))\n", "    except Exception:\n", "        pass\n", "if tab_list:\n", "    dic = tab_list[0]\n", "    for t in tab_list[1:]:\n", "        dic = dic.unionByName(t, allowMissingColumns=True)\n", "    display(dic.orderBy('tabela','coluna')) if 'display' in globals() else dic.orderBy('tabela','coluna').show(200, False)\n"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Governan√ßa de Dados & Conformidade\n", "- Minimizar exposi√ß√£o de PII: utilizar *hashes* ou *masking* para `cpfCnpj`.\n", "- Controle de acesso por ambiente/pasta em Databricks.\n", "- *Versionamento* do notebook e dados curados.\n", "- Testes de DQ em *jobs* agendados."]}, {"cell_type": "markdown", "metadata": {}, "source": ["---\n", "**FIM ‚Äì PSI Empr√©stimos | Sandro Kazanoski Bartz**\n", "\n", "> Sugest√£o: publicar *insights* e tabelas *gold* em um dashboard (Power BI / Databricks SQL) com *slicer* de per√≠odo, tipo de produto e institui√ß√£o."]}]}