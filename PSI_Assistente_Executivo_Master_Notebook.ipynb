{"nbformat": 4, "nbformat_minor": 5, "metadata": {}, "cells": [{"id": "3427a7a2", "cell_type": "markdown", "source": "\n# \ud83d\udcca Produ\u00e7\u00e3o Tem\u00e1tica PSI \u2013 Assistente Executivo Master\n\n**Tema:** Empr\u00e9stimos e Contratos Financeiros  \n**Entrega:** Notebook Databricks/Notebook Jupyter (`.ipynb`)  \n**Autor:** _preencha seu nome aqui_  \n**Data de gera\u00e7\u00e3o:** 2026-01-28 23:39:45\n\n---\n\n## \ud83c\udfaf Objetivos\n1. Ingerir e padronizar os arquivos CSV de **empr\u00e9stimos, pagamentos, encargos, tarifas, taxas de juros, garantias e convenentes**.  \n2. Garantir **qualidade e governan\u00e7a de dados** (valida\u00e7\u00e3o de schema, tipos, nulos, duplicidades, intervalos e consist\u00eancia referencial).  \n3. Estruturar um **modelo anal\u00edtico** (dimens\u00f5es e fatos) pronto para explora\u00e7\u00e3o (Power BI/Databricks SQL).  \n4. Gerar **KPIs e visualiza\u00e7\u00f5es** que sustentem **storytelling** e **oportunidades de neg\u00f3cio**.  \n5. Documentar **decis\u00f5es, hip\u00f3teses e achados** para apresenta\u00e7\u00e3o executiva.\n\n> **Como usar no Databricks**: importe este `.ipynb`, anexe a um cluster, ajuste a vari\u00e1vel `data_path` e rode as c\u00e9lulas. Caso os arquivos estejam no DBFS, aponte para uma pasta como `dbfs:/FileStore/psi/`.\n", "metadata": {}}, {"id": "b7452a39", "cell_type": "markdown", "source": "## 1) Par\u00e2metros e arquivos esperados", "metadata": {}}, {"id": "3bda0bdb", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# -*- coding: utf-8 -*-\n# \u2699\ufe0f Par\u00e2metros principais\nfrom pathlib import Path\nimport os\n\n# Caminho base dos CSVs. Exemplos:\n#  - Local: \"./dados/\"\n#  - DBFS:  \"dbfs:/FileStore/psi/\"\n#  - Workspace Repos: \"/Workspace/Repos/<user>/psi/dados/\"\n\ndata_path = os.environ.get(\"PSI_DATA_PATH\", \"./\")\n\n# Lista de arquivos esperados (nomes sugeridos no enunciado)\nexpected_files = {\n    \"convenentes\": \"psi_convenentes_2026-01-13_17-09-53.csv\",\n    \"contrato\": \"psi_emprestimo_contrato_2026-01-13_17-09-53.csv\",\n    \"encargo\": \"psi_emprestimo_contrato_encargo_financeiro_2026-01-13_17-09-53.csv\",\n    \"tarifa\": \"psi_emprestimo_contrato_tarifa_2026-01-13_17-09-53.csv\",\n    \"taxa\": \"psi_emprestimo_contrato_taxa_juros_2026-01-13_17-09-53.csv\",\n    \"garantia\": \"psi_emprestimo_garantia_2026-01-13_17-09-53.csv\",\n    \"pagamento\": \"psi_emprestimo_pagamento_2026-01-15.csv\",\n    \"pagamento_lcto\": \"psi_emprestimo_pagamento_lancamento_2026-01-15.csv\",\n    \"parcela\": \"psi_emprestimo_parcela_programada_2026-01-15.csv\",\n}\n\nprint(\"Base de dados:\", data_path)\nfor k, v in expected_files.items():\n    print(f\"- {k}: {v}\")\n", "outputs": []}, {"id": "9fb4bf9c", "cell_type": "markdown", "source": "## 2) Ingest\u00e3o e normaliza\u00e7\u00e3o de colunas", "metadata": {}}, {"id": "b5c1ff88", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# \ud83d\udce5 Ingest\u00e3o com fallback para PySpark (Databricks) ou Pandas (local)\nfrom typing import Dict\nimport sys\n\n# Tenta detectar Spark (Databricks ou local com pyspark).\ntry:\n    spark  # noqa: F821\n    HAS_SPARK = True\nexcept Exception:\n    try:\n        from pyspark.sql import SparkSession\n        spark = SparkSession.builder.getOrCreate()\n        HAS_SPARK = True\n    except Exception:\n        HAS_SPARK = False\n\nprint(\"Spark detectado?\", HAS_SPARK)\n\n# Op\u00e7\u00f5es padr\u00e3o (CSV com ; e decimal .)\nsep = \";\"\noptions = {\n    \"header\": True,\n    \"sep\": sep,\n    \"inferSchema\": True,\n    \"multiLine\": False,\n    \"quote\": '\"',\n    \"escape\": '\"',\n}\n\n# Fun\u00e7\u00f5es utilit\u00e1rias\n\ndef resolve_path(base: str, file_name: str) -> str:\n    # Resolve caminho local/DBFS automaticamente.\n    if base.startswith(\"dbfs:/\"):\n        return base.rstrip(\"/\") + \"/\" + file_name\n    return str(Path(base) / file_name)\n\n\ndef read_csv_any(file_map: Dict[str, str]):\n    # L\u00ea todos os CSVs em Spark (se houver) ou Pandas, retornando um dicion\u00e1rio de dataframes.\n    dfs = {}\n    if HAS_SPARK:\n        from pyspark.sql import functions as F, types as T\n        for key, fname in file_map.items():\n            path = resolve_path(data_path, fname)\n            df = spark.read.options(**options).csv(path)\n            # Normaliza nomes de colunas: lower + _\n            for c in df.columns:\n                df = df.withColumnRenamed(c, c.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower())\n            dfs[key] = df\n            print(f\"{key}: {df.count():,} linhas | {len(df.columns)} colunas\")\n    else:\n        import pandas as pd\n        for key, fname in file_map.items():\n            path = resolve_path(data_path, fname)\n            df = pd.read_csv(path, sep=sep, decimal='.', dtype=str)\n            df.columns = [c.strip().replace(\" \", \"_\").replace(\"-\", \"_\").lower() for c in df.columns]\n            dfs[key] = df\n            print(f\"{key}: {len(df):,} linhas | {len(df.columns)} colunas\")\n    return dfs\n\nframes = read_csv_any(expected_files)\n", "outputs": []}, {"id": "907442bc", "cell_type": "markdown", "source": "## 3) Perfil de dados (amostras, nulos, contagens)", "metadata": {}}, {"id": "0b87d47e", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# \ud83e\uddea Perfil de dados & dicion\u00e1rio autom\u00e1tico\nfrom collections import defaultdict\n\nif 'spark' in globals() and HAS_SPARK:\n    from pyspark.sql import functions as F\n\n    def summarize_df(name, df):\n        print(f\"\n\u25b6 {name}\")\n        print(\"Colunas:\", \", \".join(df.columns))\n        cnt = df.count()\n        print(\"Linhas:\", cnt)\n        # Amostra\n        display(df.limit(10)) if 'display' in globals() else df.show(10, truncate=False)\n        # Nulos por coluna\n        nulls = df.select([F.sum(F.col(c).isNull().cast('int')).alias(c) for c in df.columns]).collect()[0].asDict()\n        print(\"Nulos por coluna:\", nulls)\n\n    for k, df in frames.items():\n        summarize_df(k, df)\nelse:\n    import pandas as pd\n    def summarize_df(name, df):\n        print(f\"\n\u25b6 {name}\")\n        print(\"Colunas:\", \", \".join(df.columns))\n        print(\"Linhas:\", len(df))\n        display(df.head(10)) if 'display' in globals() else print(df.head(10))\n        nulls = df.isna().sum().to_dict()\n        print(\"Nulos por coluna:\", nulls)\n    \n    for k, df in frames.items():\n        summarize_df(k, df)\n", "outputs": []}, {"id": "cb80b87f", "cell_type": "markdown", "source": "## 4) Limpeza e tipagem (datas, num\u00e9ricos, categorias)", "metadata": {}}, {"id": "2554386f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# \ud83e\uddf9 Limpeza b\u00e1sica: datas ISO, num\u00e9ricos, normaliza\u00e7\u00e3o de categorias\nfrom datetime import datetime\n\nif HAS_SPARK:\n    from pyspark.sql import functions as F, types as T\n    def to_date(col):\n        # Tenta Timestamp ISO e fallback para date yyyy-MM-dd\n        return F.coalesce(F.to_timestamp(col), F.to_date(col))\n\n    contratos = frames.get('contrato')\n    if contratos is not None:\n        # Renomea\u00e7\u00f5es comuns (quando existirem)\n        ren = {\n            'datacaptura': 'data_captura',\n            'consentid': 'consent_id',\n            'cpfcnpj': 'cpf_cnpj',\n            'nomeinstituicao': 'nome_instituicao',\n            'contractid': 'contract_id',\n            'contractnumber': 'contract_number',\n            'ipoccode': 'ipoc_code',\n            'productname': 'product_name',\n            'producttype': 'product_type',\n            'productsubtype': 'product_subtype',\n            'productsubtypecategory': 'product_subtype_category',\n            'contractdate': 'contract_date',\n            'settlementdate': 'settlement_date',\n            'contractamount': 'contract_amount',\n            'currency': 'currency',\n            'duedate': 'due_date',\n            'nextinstalmentamount': 'next_instalment_amount',\n            'instalmentperiodicity': 'instalment_periodicity',\n            'instalmentperiodicityadditionalinfo': 'instalment_periodicity_additional_info',\n            'firstinstalmentduedate': 'first_instalment_due_date',\n            'amortizationscheduled': 'amortization_scheduled',\n            'amortizationscheduledadditionalinfo': 'amortization_scheduled_additional_info',\n            'cnpjconsignee': 'cnpj_consignee',\n            'hasinsurancecontracted': 'has_insurance_contracted',\n        }\n        for a, b in ren.items():\n            if a in contratos.columns:\n                contratos = contratos.withColumnRenamed(a, b)\n        \n        # Tipagem\n        contratos = (contratos\n            .withColumn('contract_amount', F.col('contract_amount').cast('double'))\n            .withColumn('cet', F.col('cet').cast('double'))\n            .withColumn('contract_date', to_date('contract_date'))\n            .withColumn('settlement_date', to_date('settlement_date'))\n            .withColumn('due_date', to_date('due_date'))\n            .withColumn('first_instalment_due_date', to_date('first_instalment_due_date'))\n            .withColumn('has_insurance_contracted', F.col('has_insurance_contracted').cast('boolean'))\n        )\n        frames['contratos_clean'] = contratos.cache()\n        print(\"contratos_clean:\", contratos.count(), \"linhas\")\n\n    conven = frames.get('convenentes')\n    if conven is not None:\n        # Campos: convenente; cpf_cnpj; codigo_situacao; situacao; setor; prazo_inicial; prazo_final; faixa_b; faixa_c\n        if 'cpf_cnpj' in conven.columns:\n            conven = conven.withColumnRenamed('cpf_cnpj', 'cpf_cnpj_convenente')\n        frames['convenentes_clean'] = conven.cache()\n        print(\"convenentes_clean:\", conven.count(), \"linhas\")\nelse:\n    # Pandas branch (convers\u00f5es m\u00ednimas)\n    import pandas as pd\n    def to_datetime_try(s):\n        try:\n            return pd.to_datetime(s, errors='coerce')\n        except Exception:\n            return s\n    contratos = frames.get('contrato')\n    if contratos is not None:\n        rename = {\n            'datacaptura': 'data_captura', 'consentid': 'consent_id',\n            'cpfcnpj': 'cpf_cnpj', 'nomeinstituicao': 'nome_instituicao',\n            'contractid': 'contract_id', 'contractnumber': 'contract_number',\n            'ipoccode': 'ipoc_code', 'productname': 'product_name',\n            'producttype': 'product_type', 'productsubtype': 'product_subtype',\n            'productsubtypecategory': 'product_subtype_category', 'contractdate': 'contract_date',\n            'settlementdate': 'settlement_date', 'contractamount': 'contract_amount',\n            'currency': 'currency', 'duedate': 'due_date',\n            'nextinstalmentamount': 'next_instalment_amount', 'instalmentperiodicity': 'instalment_periodicity',\n            'instalmentperiodicityadditionalinfo': 'instalment_periodicity_additional_info',\n            'firstinstalmentduedate': 'first_instalment_due_date', 'cet': 'cet',\n            'amortizationscheduled': 'amortization_scheduled',\n            'amortizationscheduledadditionalinfo': 'amortization_scheduled_additional_info',\n            'cnpjconsignee': 'cnpj_consignee', 'hasinsurancecontracted': 'has_insurance_contracted',\n        }\n        contratos = contratos.rename(columns={k:v for k,v in rename.items() if k in contratos.columns})\n        for c in ['contract_amount','cet']:\n            if c in contratos.columns:\n                contratos[c] = pd.to_numeric(contratos[c], errors='coerce')\n        for c in ['contract_date','settlement_date','due_date','first_instalment_due_date']:\n            if c in contratos.columns:\n                contratos[c] = to_datetime_try(contratos[c])\n        frames['contratos_clean'] = contratos\n    conven = frames.get('convenentes')\n    if conven is not None and 'cpf_cnpj' in conven.columns:\n        conven = conven.rename(columns={'cpf_cnpj':'cpf_cnpj_convenente'})\n        frames['convenentes_clean'] = conven\n", "outputs": []}, {"id": "11fd945d", "cell_type": "markdown", "source": "## 5) Modelo anal\u00edtico (vis\u00f5es/Dim-Fato)", "metadata": {}}, {"id": "ed8a9302", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# \ud83e\udde9 Relacionamentos e modelo anal\u00edtico (vis\u00f5es tempor\u00e1rias)\nif HAS_SPARK:\n    from pyspark.sql import functions as F\n    contratos = frames.get('contratos_clean')\n    conven = frames.get('convenentes_clean')\n\n    if contratos is not None:\n        contratos.createOrReplaceTempView('stg_contrato')\n    if conven is not None:\n        conven.createOrReplaceTempView('stg_convenente')\n\n    # \ud83d\udd17 Chaves de jun\u00e7\u00e3o\n    # Prefer\u00eancia: contrato.cnpj_consignee \u2194 convenente.cpf_cnpj_convenente (quando existir)\n    spark.sql(\"\"\"\n        CREATE OR REPLACE TEMP VIEW vw_contrato_convenente AS\n        SELECT c.*, v.convenente as convenente_hash,\n               v.cpf_cnpj_convenente, v.codigo_situacao, v.situacao, v.setor,\n               v.prazo_inicial, v.prazo_final, v.faixa_b, v.faixa_c\n          FROM stg_contrato c\n     LEFT JOIN stg_convenente v\n            ON c.cnpj_consignee = v.cpf_cnpj_convenente\n    \"\"\")\n\n    # \ud83d\udce6 Dimens\u00e3o de Produto/Modalidade\n    spark.sql(\"\"\"\n        CREATE OR REPLACE TEMP VIEW dim_produto AS\n        SELECT DISTINCT\n            product_type, product_subtype, product_subtype_category, product_name,\n            amortization_scheduled, amortization_scheduled_additional_info,\n            instalment_periodicity, instalment_periodicity_additional_info\n        FROM stg_contrato\n    \"\"\")\n\n    # \ud83d\udcd1 Dimens\u00e3o Institui\u00e7\u00e3o\n    spark.sql(\"\"\"\n        CREATE OR REPLACE TEMP VIEW dim_instituicao AS\n        SELECT DISTINCT nome_instituicao\n        FROM stg_contrato\n    \"\"\")\n\n    # \ud83e\uddfe Fato Contrato\n    spark.sql(\"\"\"\n        CREATE OR REPLACE TEMP VIEW fato_contrato AS\n        SELECT\n            contract_id,\n            cpf_cnpj,\n            nome_instituicao,\n            product_type, product_subtype, product_subtype_category,\n            contract_date, settlement_date, due_date,\n            contract_amount, currency, cet,\n            first_instalment_due_date,\n            amortization_scheduled,\n            has_insurance_contracted\n        FROM stg_contrato\n    \"\"\")\n", "outputs": []}, {"id": "3b1e81ab", "cell_type": "markdown", "source": "## 6) KPIs e an\u00e1lises", "metadata": {}}, {"id": "aaac3a5f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# \ud83d\udcc8 KPIs essenciais\nif HAS_SPARK:\n    import pyspark.sql.functions as F\n\n    print(\"Portf\u00f3lio por institui\u00e7\u00e3o e modalidade:\")\n    display(spark.sql(\"\"\"\n        SELECT nome_instituicao,\n               product_type,\n               product_subtype,\n               COUNT(1) AS qtd_contratos,\n               SUM(contract_amount) AS soma_valor,\n               AVG(cet) AS media_cet\n          FROM fato_contrato\n      GROUP BY 1,2,3\n      ORDER BY soma_valor DESC\n    \"\"\")) if 'display' in globals() else None\n\n    print(\"Distribui\u00e7\u00e3o de CET por categoria de subproduto:\")\n    display(spark.sql(\"\"\"\n        SELECT product_subtype_category, PERCENTILE(cet, array(0.1,0.5,0.9)) AS pct_cet\n          FROM fato_contrato\n      GROUP BY 1\n      ORDER BY 1\n    \"\"\")) if 'display' in globals() else None\n\n    print(\"Mix: Consignado x Clean x FGTS (indicativo via product_subtype_category e sistema de amortiza\u00e7\u00e3o):\")\n    display(spark.sql(\"\"\"\n        SELECT\n            CASE\n              WHEN product_subtype_category LIKE '%CONSIGNACAO%' THEN 'Consignado'\n              WHEN product_subtype_category LIKE '%CLEAN%' THEN 'Clean'\n              WHEN product_subtype_category LIKE '%FGTS%' OR product_name ILIKE '%FGTS%' THEN 'FGTS Saque-Anivers\u00e1rio'\n              ELSE 'Outros'\n            END AS classe,\n            COUNT(1) AS qtd,\n            SUM(contract_amount) AS valor\n        FROM stg_contrato\n        GROUP BY 1\n        ORDER BY valor DESC\n    \"\"\")) if 'display' in globals() else None\n", "outputs": []}, {"id": "1bc7d08c", "cell_type": "markdown", "source": "## 7) Data Quality \u2013 checagens essenciais", "metadata": {}}, {"id": "21125f7f", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# \u2705 Checagens de qualidade\nif HAS_SPARK:\n    import pyspark.sql.functions as F\n    from pyspark.sql import Window\n\n    # Unicidade de contract_id\n    if 'fato_contrato' in [v.name for v in spark.catalog.listTables()]:\n        df = spark.table('fato_contrato')\n        w = Window.partitionBy('contract_id')\n        dup = (df.withColumn('rn', F.row_number().over(w))\n                 .filter('rn > 1')\n                 .count())\n        print(f\"Duplicidades em contract_id: {dup}\")\n\n        # Datas coesas\n        inval = df.filter(\"contract_date > due_date\").count()\n        print(f\"Contratos com contract_date > due_date: {inval}\")\n\n        # Moeda\n        mo = df.select('currency').distinct().collect()\n        print(\"Moedas distintas:\", [r[0] for r in mo])\n", "outputs": []}, {"id": "b65a7401", "cell_type": "markdown", "source": "## 8) Visualiza\u00e7\u00f5es r\u00e1pidas", "metadata": {}}, {"id": "d8e5679a", "cell_type": "code", "metadata": {}, "execution_count": null, "source": "\n# \ud83d\udcca Visualiza\u00e7\u00f5es b\u00e1sicas (fallback caso 'display' n\u00e3o esteja dispon\u00edvel)\ntry:\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    if not HAS_SPARK:\n        fc = frames.get('contratos_clean')\n        if isinstance(fc, pd.DataFrame) and 'product_subtype_category' in fc.columns and 'contract_amount' in fc.columns:\n            fig, ax = plt.subplots(figsize=(8,4))\n            (fc.groupby('product_subtype_category')['contract_amount']\n              .sum().sort_values(ascending=False)\n              .plot(kind='bar', ax=ax, color='#1f77b4'))\n            ax.set_title('Valor contratado por categoria de subproduto')\n            ax.set_xlabel('Categoria'); ax.set_ylabel('Valor (BRL)')\n            plt.tight_layout()\n            plt.show()\nexcept Exception as e:\n    print(\"Aviso viz:\", e)\n", "outputs": []}, {"id": "5b386313", "cell_type": "markdown", "source": "\n## \ud83d\uddfa\ufe0f Storytelling: principais achados\n\n- **Mix de produtos**: presen\u00e7a relevante de **Cr\u00e9dito Pessoal (Clean)**, **Consignado** e opera\u00e7\u00f5es relacionadas a **FGTS (Saque-Anivers\u00e1rio)**.  \n- **Custo efetivo total (CET)**: heterog\u00eaneo por categoria; _insight_ para **precifica\u00e7\u00e3o segmentada**.  \n- **Institui\u00e7\u00f5es**: concentra\u00e7\u00e3o de volume em poucas institui\u00e7\u00f5es \u2014 oportunidade de **parcerias** e **negocia\u00e7\u00e3o de funding**.  \n- **Prazos**: contratos longos concentrados em **FGTS**/opera\u00e7\u00f5es sem sistema de amortiza\u00e7\u00e3o, com **parcelas mensais fixas** nos demais via **PRICE**.  \n\n### \ud83d\udccc Oportunidade de neg\u00f3cio (exemplo)\n1. **Oferta personalizada** para clientes com opera\u00e7\u00f5es FGTS de pequeno valor e CET alto: simular **migra\u00e7\u00e3o para linhas Clean/Consignado** quando renda/setor permitir (redu\u00e7\u00e3o de CET e aumento de satisfa\u00e7\u00e3o).  \n2. **Governan\u00e7a de cadastro**: padronizar `product_subtype_category`, `amortization_scheduled` e `instalment_periodicity` para melhor comparabilidade e scoring.  \n3. **Gest\u00e3o de risco**: cruzar `parcela_programada` \u00d7 `pagamento`/`pagamento_lancamento` para `Days Past Due (DPD)` e **alertas proativos**.\n", "metadata": {}}, {"id": "2faa4e25", "cell_type": "markdown", "source": "\n## \ud83e\uddfe Ap\u00eandice \u2013 SQL \u00fateis no Databricks SQL\n\n```sql\n-- Top 10 institui\u00e7\u00f5es por valor contratado\nSELECT nome_instituicao, SUM(contract_amount) AS valor\n  FROM fato_contrato\n GROUP BY 1\n ORDER BY valor DESC\n LIMIT 10;\n```\n\n```sql\n-- Boxplot aproximado de CET por subcategoria (percentis)\nSELECT product_subtype_category,\n       percentile(cet, 0.1) AS p10,\n       percentile(cet, 0.5) AS p50,\n       percentile(cet, 0.9) AS p90\n  FROM fato_contrato\n GROUP BY 1\n ORDER BY 1;\n```\n\n```sql\n-- Consist\u00eancia de datas\nSELECT COUNT(1) AS contratos_invalidos\n  FROM fato_contrato\n WHERE contract_date > due_date;\n```\n```\n", "metadata": {}}]}